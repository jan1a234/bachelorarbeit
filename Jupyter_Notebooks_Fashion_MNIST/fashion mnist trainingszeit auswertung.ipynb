{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3db075-d57f-4e18-b2ab-5aa365dc62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ZUSAMMENFASSUNG DER TRAININGSZEITEN\n",
      "================================================================================\n",
      "\n",
      "Durchschnittliche Trainingszeiten nach Klassifikator (in Sekunden):\n",
      "------------------------------------------------------------\n",
      "                        mean     std     min      max\n",
      "classifier                                           \n",
      "CNN                   4.6101  2.1901  1.5018   7.8621\n",
      "Logistic Regression   0.9130  0.3373  0.4304   1.4005\n",
      "Naive Bayes           0.1298  0.0604  0.0439   0.2173\n",
      "Random Forest         0.2362  0.0346  0.1810   0.2921\n",
      "SVM                  11.2627  4.6274  5.5903  19.8647\n",
      "\n",
      "\n",
      "Durchschnittliche Trainingszeiten nach Query-Strategie (in Sekunden):\n",
      "------------------------------------------------------------\n",
      "                       mean     std     min      max\n",
      "strategy_de                                         \n",
      "Entropie-Auswahl     3.3164  4.7994  0.0441  19.8647\n",
      "Geringste Konfidenz  3.8148  5.3964  0.0439  19.8495\n",
      "Margin-Auswahl       3.7870  5.3817  0.0441  19.8464\n",
      "Zufallsauswahl       2.8032  3.4544  0.0445  12.1750\n",
      "\n",
      "\n",
      "Skalierungsfaktor (60k vs 12k Daten) nach Klassifikator:\n",
      "------------------------------------------------------------\n",
      "CNN                 : 5.02x\n",
      "Logistic Regression : 3.12x\n",
      "Naive Bayes         : 4.82x\n",
      "Random Forest       : 1.52x\n",
      "SVM                 : 2.72x\n",
      "\n",
      "\n",
      "Effizienteste Kombination (Top 5 schnellste):\n",
      "------------------------------------------------------------\n",
      "Naive Bayes          | Margin-Auswahl       |  12000 Daten | 0.0444 Sek\n",
      "Naive Bayes          | Geringste Konfidenz  |  12000 Daten | 0.0444 Sek\n",
      "Naive Bayes          | Entropie-Auswahl     |  12000 Daten | 0.0445 Sek\n",
      "Naive Bayes          | Zufallsauswahl       |  12000 Daten | 0.0450 Sek\n",
      "Naive Bayes          | Margin-Auswahl       |  24000 Daten | 0.0872 Sek\n",
      "Naive Bayes          | Zufallsauswahl       |  24000 Daten | 0.0876 Sek\n",
      "Naive Bayes          | Entropie-Auswahl     |  24000 Daten | 0.0876 Sek\n",
      "Naive Bayes          | Geringste Konfidenz  |  24000 Daten | 0.0878 Sek\n",
      "Naive Bayes          | Zufallsauswahl       |  36000 Daten | 0.1283 Sek\n",
      "Naive Bayes          | Margin-Auswahl       |  36000 Daten | 0.1297 Sek\n",
      "\n",
      "\n",
      "Langsamste Kombination (Top 5 langsamste):\n",
      "------------------------------------------------------------\n",
      "SVM                  | Geringste Konfidenz  |  60000 Daten | 19.8113 Sek\n",
      "SVM                  | Margin-Auswahl       |  60000 Daten | 19.7884 Sek\n",
      "SVM                  | Entropie-Auswahl     |  60000 Daten | 19.7850 Sek\n",
      "SVM                  | Geringste Konfidenz  |  48000 Daten | 15.2306 Sek\n",
      "SVM                  | Margin-Auswahl       |  48000 Daten | 15.2112 Sek\n",
      "SVM                  | Entropie-Auswahl     |  48000 Daten | 12.7619 Sek\n",
      "SVM                  | Geringste Konfidenz  |  36000 Daten | 12.7203 Sek\n",
      "SVM                  | Margin-Auswahl       |  36000 Daten | 12.6928 Sek\n",
      "SVM                  | Zufallsauswahl       |  60000 Daten | 12.1185 Sek\n",
      "SVM                  | Geringste Konfidenz  |  24000 Daten | 10.1777 Sek\n",
      "\n",
      "================================================================================\n",
      "PDF 'active_learning_trainingszeiten.pdf' wurde erfolgreich erstellt!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib für deutsche Beschriftungen konfigurieren\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "\n",
    "# Seaborn Style setzen\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Daten manuell eingeben basierend auf den PDFs\n",
    "# Die Daten aus den 5 PDFs repräsentieren die 5 Klassifikatoren\n",
    "\n",
    "# Daten für CNN (Dokument 1)\n",
    "data_cnn = {\n",
    "    'strategy': ['Random Sampling']*25 + ['Entropy Sampling']*25 + ['Margin Sampling']*25 + ['Least Confidence']*25,\n",
    "    'budget_pct': ([0.2]*5 + [0.4]*5 + [0.6]*5 + [0.8]*5 + [1.0]*5) * 4,\n",
    "    'run': list(range(5)) * 20,\n",
    "    'n_labeled': ([12000]*5 + [24000]*5 + [36000]*5 + [48000]*5 + [60000]*5) * 4,\n",
    "    'avg_train_time': [\n",
    "        # Random Sampling\n",
    "        1.521085076, 1.502775908, 1.501781723, 1.502621081, 1.502706331,\n",
    "        2.97725921, 2.983116505, 2.997533179, 3.012997597, 3.020966895,\n",
    "        4.5361837, 4.544467459, 4.550611335, 4.553623045, 4.556777531,\n",
    "        6.068494465, 6.071016299, 6.071812213, 6.073508913, 6.076823483,\n",
    "        7.592866254, 7.595250643, 7.596366393, 7.597735401, 7.597040092,\n",
    "        # Entropy Sampling\n",
    "        1.533692256, 1.529967018, 1.530879539, 1.530919199, 1.529847155,\n",
    "        3.042272654, 3.044967428, 3.04630348, 3.044037535, 3.042879394,\n",
    "        4.562415472, 4.564509718, 4.564144403, 4.56408135, 4.562399176,\n",
    "        6.083620132, 6.083112973, 6.082519689, 6.081528182, 6.083057825,\n",
    "        7.606978691, 7.605105717, 7.633382693, 7.662749339, 7.662503158,\n",
    "        # Margin Sampling\n",
    "        1.544966718, 1.544624308, 1.541594868, 1.541721904, 1.543142764,\n",
    "        3.068383024, 3.070673745, 3.069041729, 3.071545784, 3.069628832,\n",
    "        4.599050807, 4.599122638, 4.600143036, 4.598187269, 4.599487278,\n",
    "        6.128177532, 6.126837956, 6.132447323, 6.15017252, 6.154459896,\n",
    "        7.693038536, 7.693774452, 7.691228987, 7.689596619, 7.693505323,\n",
    "        # Least Confidence\n",
    "        1.549689438, 1.549501222, 1.550653592, 1.551303781, 1.581333907,\n",
    "        3.159619839, 3.156467524, 3.156243116, 3.154623001, 3.154910296,\n",
    "        4.719349008, 4.721554813, 4.723619152, 4.723466504, 4.722391434,\n",
    "        6.290463124, 6.289522076, 6.288772335, 6.292358554, 6.29081067,\n",
    "        7.862094092, 7.861531244, 7.856163269, 7.856267458, 7.854389507\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Daten für Logistic Regression (Dokument 2)\n",
    "data_lr = {\n",
    "    'strategy': ['Random Sampling']*25 + ['Entropy Sampling']*25 + ['Margin Sampling']*25 + ['Least Confidence']*25,\n",
    "    'budget_pct': ([0.2]*5 + [0.4]*5 + [0.6]*5 + [0.8]*5 + [1.0]*5) * 4,\n",
    "    'run': list(range(5)) * 20,\n",
    "    'n_labeled': ([12000]*5 + [24000]*5 + [36000]*5 + [48000]*5 + [60000]*5) * 4,\n",
    "    'avg_train_time': [\n",
    "        # Random Sampling\n",
    "        0.430414107, 0.434070763, 0.437924333, 0.442383891, 0.44850282,\n",
    "        0.666724175, 0.663643624, 0.666885457, 0.666870538, 0.66949228,\n",
    "        0.906450863, 0.905770513, 0.906783383, 0.90681656, 0.91027872,\n",
    "        1.153505619, 1.153761931, 1.155046857, 1.157732933, 1.158681829,\n",
    "        1.40018023, 1.399510408, 1.397971267, 1.393453348, 1.394937257,\n",
    "        # Entropy Sampling\n",
    "        0.44753084, 0.443597534, 0.446408096, 0.445562954, 0.448979077,\n",
    "        0.669268989, 0.668161316, 0.67034522, 0.672580425, 0.673893269,\n",
    "        0.902393724, 0.903276306, 0.903596949, 0.902608667, 0.905928283,\n",
    "        1.142473753, 1.144409172, 1.144791229, 1.143618097, 1.148521042,\n",
    "        1.386908441, 1.388435424, 1.391244007, 1.388608534, 1.391419936,\n",
    "        # Margin Sampling\n",
    "        0.449506324, 0.446377806, 0.450042009, 0.449820829, 0.451139512,\n",
    "        0.67007042, 0.667958665, 0.670687914, 0.670846893, 0.670404673,\n",
    "        0.900362717, 0.902063168, 0.905314432, 0.903093623, 0.904682129,\n",
    "        1.141420533, 1.145052533, 1.14466871, 1.144293281, 1.147952557,\n",
    "        1.387121649, 1.388144371, 1.390303175, 1.387223406, 1.389719134,\n",
    "        # Least Confidence\n",
    "        0.452533763, 0.450354348, 0.446699371, 0.449374271, 0.453598955,\n",
    "        0.670029097, 0.67045781, 0.675982176, 0.673277388, 0.676585908,\n",
    "        0.910121202, 0.907668372, 0.910441033, 0.909627055, 0.912553582,\n",
    "        1.152339536, 1.150865906, 1.150365531, 1.15430273, 1.150210135,\n",
    "        1.3962539, 1.397003805, 1.398311481, 1.395124504, 1.400465492\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Daten für Naive Bayes (Dokument 3 - sehr kleine Zeiten)\n",
    "data_nb = {\n",
    "    'strategy': ['Random Sampling']*25 + ['Entropy Sampling']*25 + ['Margin Sampling']*25 + ['Least Confidence']*25,\n",
    "    'budget_pct': ([0.2]*5 + [0.4]*5 + [0.6]*5 + [0.8]*5 + [1.0]*5) * 4,\n",
    "    'run': list(range(5)) * 20,\n",
    "    'n_labeled': ([12000]*5 + [24000]*5 + [36000]*5 + [48000]*5 + [60000]*5) * 4,\n",
    "    'avg_train_time': [\n",
    "        # Random Sampling\n",
    "        0.044904242, 0.045427913, 0.045572312, 0.044479152, 0.044497075,\n",
    "        0.085929891, 0.087938755, 0.088246969, 0.086517978, 0.089378403,\n",
    "        0.128625044, 0.128465757, 0.128066493, 0.126750429, 0.129766797,\n",
    "        0.172569523, 0.172209065, 0.173458007, 0.171681811, 0.17276723,\n",
    "        0.215535144, 0.216414027, 0.213230311, 0.213274355, 0.215519274,\n",
    "        # Entropy Sampling\n",
    "        0.044630714, 0.044069798, 0.044171157, 0.044869796, 0.044918496,\n",
    "        0.088534279, 0.088299097, 0.086302138, 0.086233722, 0.088829167,\n",
    "        0.130589102, 0.130465763, 0.131825827, 0.130653297, 0.128563243,\n",
    "        0.173025558, 0.176241031, 0.171646522, 0.17273794, 0.173139507,\n",
    "        0.216089974, 0.213906793, 0.214336381, 0.215689477, 0.21318728,\n",
    "        # Margin Sampling\n",
    "        0.044133394, 0.044300017, 0.044421072, 0.044250955, 0.044895411,\n",
    "        0.087127107, 0.086885615, 0.08565505, 0.088557522, 0.087548271,\n",
    "        0.129344158, 0.1297024, 0.12924326, 0.130578421, 0.129503398,\n",
    "        0.171091529, 0.172271104, 0.171494788, 0.170917556, 0.171385983,\n",
    "        0.213984505, 0.212621154, 0.214580682, 0.213818883, 0.214850883,\n",
    "        # Least Confidence\n",
    "        0.044761658, 0.044804884, 0.043863711, 0.044454968, 0.044124821,\n",
    "        0.088369187, 0.086711336, 0.088337807, 0.087724036, 0.08805419,\n",
    "        0.129984503, 0.130504967, 0.129831418, 0.130670269, 0.13072482,\n",
    "        0.172372896, 0.172822473, 0.171587184, 0.172248715, 0.172725286,\n",
    "        0.217317597, 0.216626286, 0.215034569, 0.213721053, 0.214255287\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Daten für Random Forest (Dokument 4)\n",
    "data_rf = {\n",
    "    'strategy': ['Random Sampling']*25 + ['Entropy Sampling']*25 + ['Margin Sampling']*25 + ['Least Confidence']*25,\n",
    "    'budget_pct': ([0.2]*5 + [0.4]*5 + [0.6]*5 + [0.8]*5 + [1.0]*5) * 4,\n",
    "    'run': list(range(5)) * 20,\n",
    "    'n_labeled': ([12000]*5 + [24000]*5 + [36000]*5 + [48000]*5 + [60000]*5) * 4,\n",
    "    'avg_train_time': [\n",
    "        # Random Sampling\n",
    "        0.181024375, 0.181443173, 0.181925473, 0.182831526, 0.182940079,\n",
    "        0.209142624, 0.204839443, 0.20475943, 0.206234283, 0.209145926,\n",
    "        0.231014534, 0.231087369, 0.231550378, 0.231923066, 0.23275368,\n",
    "        0.261499407, 0.25845696, 0.260742654, 0.258089964, 0.259150809,\n",
    "        0.279392623, 0.278795467, 0.279237723, 0.281439607, 0.285700948,\n",
    "        # Entropy Sampling\n",
    "        0.189325571, 0.188209254, 0.189695483, 0.196063819, 0.189229022,\n",
    "        0.214630142, 0.213050604, 0.21443714, 0.21203073, 0.212053624,\n",
    "        0.235228374, 0.235411547, 0.235309104, 0.237508985, 0.235569195,\n",
    "        0.262416443, 0.261908689, 0.261195991, 0.259595264, 0.258364911,\n",
    "        0.28291877, 0.282267979, 0.282410894, 0.282449556, 0.285054654,\n",
    "        # Margin Sampling\n",
    "        0.189271616, 0.187176829, 0.188204071, 0.187763297, 0.190631379,\n",
    "        0.21764808, 0.21725611, 0.213284726, 0.215038132, 0.21258655,\n",
    "        0.237819007, 0.236781735, 0.23719843, 0.236060928, 0.235635825,\n",
    "        0.259415494, 0.25997328, 0.258534948, 0.257463741, 0.258455475,\n",
    "        0.280901923, 0.283022277, 0.2842962, 0.284526024, 0.285941531,\n",
    "        # Least Confidence\n",
    "        0.186542625, 0.187671703, 0.187047585, 0.191757814, 0.187757471,\n",
    "        0.213432155, 0.213734201, 0.21515722, 0.213531854, 0.214701226,\n",
    "        0.24190396, 0.241706741, 0.24122898, 0.241975929, 0.241126111,\n",
    "        0.262677755, 0.264882198, 0.265062676, 0.262004348, 0.267042474,\n",
    "        0.292114174, 0.28661645, 0.288433734, 0.287224451, 0.289441369\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Daten für SVM (Dokument 5)\n",
    "data_svm = {\n",
    "    'strategy': ['Random Sampling']*25 + ['Entropy Sampling']*25 + ['Margin Sampling']*25 + ['Least Confidence']*25,\n",
    "    'budget_pct': ([0.2]*5 + [0.4]*5 + [0.6]*5 + [0.8]*5 + [1.0]*5) * 4,\n",
    "    'run': list(range(5)) * 20,\n",
    "    'n_labeled': ([12000]*5 + [24000]*5 + [36000]*5 + [48000]*5 + [60000]*5) * 4,\n",
    "    'avg_train_time': [\n",
    "        # Random Sampling\n",
    "        5.59030737, 5.631493206, 5.637021417, 5.714793775, 5.692046165,\n",
    "        6.680006372, 6.678584266, 6.655204053, 6.704958682, 6.669765878,\n",
    "        7.703251607, 7.709751032, 7.695497489, 7.716707777, 7.718714835,\n",
    "        8.827618975, 8.825608959, 8.809704961, 8.846616552, 8.819319994,\n",
    "        12.17497655, 12.0745011, 12.10531111, 12.07346025, 12.16441602,\n",
    "        # Entropy Sampling\n",
    "        5.791202027, 5.759870301, 5.784416054, 5.787054186, 5.769426107,\n",
    "        7.090640139, 7.07274338, 7.065608319, 7.074797514, 7.070599307,\n",
    "        8.271367547, 8.267165543, 8.263351273, 8.267215151, 8.252499574,\n",
    "        9.469496285, 9.45080651, 14.47313201, 15.22126127, 15.19501162,\n",
    "        19.86465102, 19.80036875, 19.78375989, 19.73566187, 19.74067761,\n",
    "        # Margin Sampling\n",
    "        7.443813708, 7.377446714, 7.40087108, 7.412813207, 7.401774406,\n",
    "        10.13653782, 10.16182711, 10.14165094, 10.15075541, 10.1491361,\n",
    "        12.67734949, 12.71951891, 12.68432982, 12.69772379, 12.68531133,\n",
    "        15.2147598, 15.23573265, 15.19877471, 15.19631885, 15.21041072,\n",
    "        19.79250878, 19.8052629, 19.84635638, 19.76931348, 19.72871738,\n",
    "        # Least Confidence\n",
    "        7.410335292, 7.416148144, 7.402038149, 7.42436493, 7.435452969,\n",
    "        10.17758531, 10.17286485, 10.18398625, 10.17652081, 10.17764466,\n",
    "        12.7190653, 12.73554912, 12.70581536, 12.71939814, 12.72156518,\n",
    "        15.22794602, 15.24144237, 15.22603206, 15.22580796, 15.23181833,\n",
    "        19.75822038, 19.84954281, 19.82456296, 19.77718003, 19.84717054\n",
    "    ]\n",
    "}\n",
    "\n",
    "# DataFrames erstellen\n",
    "df_cnn = pd.DataFrame(data_cnn)\n",
    "df_cnn['classifier'] = 'CNN'\n",
    "\n",
    "df_lr = pd.DataFrame(data_lr)\n",
    "df_lr['classifier'] = 'Logistic Regression'\n",
    "\n",
    "df_nb = pd.DataFrame(data_nb)\n",
    "df_nb['classifier'] = 'Naive Bayes'\n",
    "\n",
    "df_rf = pd.DataFrame(data_rf)\n",
    "df_rf['classifier'] = 'Random Forest'\n",
    "\n",
    "df_svm = pd.DataFrame(data_svm)\n",
    "df_svm['classifier'] = 'SVM'\n",
    "\n",
    "# Alle Daten kombinieren\n",
    "df_all = pd.concat([df_cnn, df_lr, df_nb, df_rf, df_svm], ignore_index=True)\n",
    "\n",
    "# Deutsche Übersetzungen für Strategien\n",
    "strategy_translation = {\n",
    "    'Random Sampling': 'Zufallsauswahl',\n",
    "    'Entropy Sampling': 'Entropie-Auswahl',\n",
    "    'Margin Sampling': 'Margin-Auswahl',\n",
    "    'Least Confidence': 'Geringste Konfidenz'\n",
    "}\n",
    "\n",
    "df_all['strategy_de'] = df_all['strategy'].map(strategy_translation)\n",
    "\n",
    "# Statistiken berechnen\n",
    "df_stats = df_all.groupby(['classifier', 'strategy_de', 'n_labeled'])['avg_train_time'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# PDF für alle Plots erstellen\n",
    "pdf_pages = PdfPages('active_learning_trainingszeiten.pdf')\n",
    "\n",
    "# Plot 1: Trainingszeiten nach Klassifikator und Strategie\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Active Learning: Trainingszeiten nach Klassifikator und Query-Strategie', fontsize=16, y=1.02)\n",
    "\n",
    "classifiers = ['CNN', 'Logistic Regression', 'Naive Bayes', 'Random Forest', 'SVM']\n",
    "colors = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for idx, classifier in enumerate(classifiers):\n",
    "    ax = axes.flatten()[idx]\n",
    "    df_classifier = df_all[df_all['classifier'] == classifier]\n",
    "    \n",
    "    for i, strategy in enumerate(df_classifier['strategy_de'].unique()):\n",
    "        df_strategy = df_classifier[df_classifier['strategy_de'] == strategy]\n",
    "        df_grouped = df_strategy.groupby('n_labeled')['avg_train_time'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        ax.errorbar(df_grouped['n_labeled'], df_grouped['mean'], yerr=df_grouped['std'], \n",
    "                   label=strategy, marker='o', capsize=5, capthick=2, linewidth=2, \n",
    "                   markersize=8, alpha=0.8, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Anzahl gelabelter Daten', fontsize=11)\n",
    "    ax.set_ylabel('Trainingszeit (Sekunden)', fontsize=11)\n",
    "    ax.set_title(f'{classifier}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(8000, 64000)\n",
    "    \n",
    "    # Y-Achse dynamisch skalieren\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    y_range = y_max - y_min\n",
    "    if y_range < 1:  # Für sehr kleine Unterschiede\n",
    "        ax.set_ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "\n",
    "# Leeres Subplot entfernen\n",
    "axes.flatten()[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Vergleich aller Klassifikatoren pro Strategie\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Trainingszeiten: Vergleich der Klassifikatoren nach Query-Strategie', fontsize=16, y=1.02)\n",
    "\n",
    "strategies_de = df_all['strategy_de'].unique()\n",
    "classifier_colors = sns.color_palette(\"Set2\", 5)\n",
    "\n",
    "for idx, strategy in enumerate(strategies_de):\n",
    "    ax = axes.flatten()[idx]\n",
    "    df_strategy = df_all[df_all['strategy_de'] == strategy]\n",
    "    \n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        df_clf = df_strategy[df_strategy['classifier'] == classifier]\n",
    "        df_grouped = df_clf.groupby('n_labeled')['avg_train_time'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        ax.errorbar(df_grouped['n_labeled'], df_grouped['mean'], yerr=df_grouped['std'],\n",
    "                   label=classifier, marker='o', capsize=3, linewidth=2,\n",
    "                   markersize=7, alpha=0.8, color=classifier_colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Anzahl gelabelter Daten', fontsize=11)\n",
    "    ax.set_ylabel('Trainingszeit (Sekunden)', fontsize=11)\n",
    "    ax.set_title(f'{strategy}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(8000, 64000)\n",
    "    \n",
    "    # Logarithmische Skala für bessere Darstellung der Unterschiede\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot 3: Heatmap der durchschnittlichen Trainingszeiten\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Pivot-Tabelle für Heatmap erstellen\n",
    "pivot_data = df_all.pivot_table(values='avg_train_time', \n",
    "                                index=['classifier'], \n",
    "                                columns=['strategy_de', 'n_labeled'], \n",
    "                                aggfunc='mean')\n",
    "\n",
    "# Spalten neu ordnen für bessere Lesbarkeit\n",
    "columns_order = []\n",
    "for strategy in strategies_de:\n",
    "    for n_labeled in sorted(df_all['n_labeled'].unique()):\n",
    "        columns_order.append((strategy, n_labeled))\n",
    "\n",
    "pivot_data = pivot_data[columns_order]\n",
    "\n",
    "# Heatmap erstellen mit logarithmischer Farbskala\n",
    "im = ax.imshow(np.log10(pivot_data.values), cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "ax.set_xticks(range(len(pivot_data.columns)))\n",
    "ax.set_xticklabels([f'{s}\\n{n//1000}k' for s, n in pivot_data.columns], rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticks(range(len(pivot_data.index)))\n",
    "ax.set_yticklabels(pivot_data.index, fontsize=11)\n",
    "\n",
    "# Colorbar mit korrekten Labels\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Log10(Trainingszeit in Sekunden)', fontsize=11)\n",
    "\n",
    "# Titel\n",
    "ax.set_title('Heatmap: Logarithmische Trainingszeiten nach Klassifikator und Strategie', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot 4: Boxplots für Variabilität der Trainingszeiten\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Variabilität der Trainingszeiten', fontsize=16, y=1.02)\n",
    "\n",
    "# Boxplot nach Klassifikator\n",
    "ax1 = axes[0]\n",
    "df_melted = df_all[['classifier', 'avg_train_time']].copy()\n",
    "classifiers_sorted = df_melted.groupby('classifier')['avg_train_time'].median().sort_values().index\n",
    "sns.boxplot(data=df_melted, y='classifier', x='avg_train_time', order=classifiers_sorted, ax=ax1, palette='Set2')\n",
    "ax1.set_xlabel('Trainingszeit (Sekunden)', fontsize=11)\n",
    "ax1.set_ylabel('Klassifikator', fontsize=11)\n",
    "ax1.set_title('Verteilung nach Klassifikator', fontsize=13, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Boxplot nach Strategie\n",
    "ax2 = axes[1]\n",
    "df_melted2 = df_all[['strategy_de', 'avg_train_time']].copy()\n",
    "sns.boxplot(data=df_melted2, x='strategy_de', y='avg_train_time', ax=ax2, palette='husl')\n",
    "ax2.set_xlabel('Query-Strategie', fontsize=11)\n",
    "ax2.set_ylabel('Trainingszeit (Sekunden)', fontsize=11)\n",
    "ax2.set_title('Verteilung nach Query-Strategie', fontsize=13, fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot 5: Skalierungsverhalten (Trainingszeit vs. Datenmenge)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Durchschnittliche Trainingszeit pro Klassifikator und Datenmenge\n",
    "for classifier in classifiers:\n",
    "    df_clf = df_all[df_all['classifier'] == classifier]\n",
    "    df_grouped = df_clf.groupby('n_labeled')['avg_train_time'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    ax.plot(df_grouped['n_labeled'], df_grouped['mean'], marker='o', \n",
    "           markersize=10, linewidth=3, label=classifier, alpha=0.8)\n",
    "    \n",
    "    # Trendlinie hinzufügen\n",
    "    z = np.polyfit(df_grouped['n_labeled'], df_grouped['mean'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df_grouped['n_labeled'], p(df_grouped['n_labeled']), \n",
    "           linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Anzahl gelabelter Daten', fontsize=12)\n",
    "ax.set_ylabel('Durchschnittliche Trainingszeit (Sekunden)', fontsize=12)\n",
    "ax.set_title('Skalierungsverhalten der Klassifikatoren', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', frameon=True, fancybox=True, shadow=True, fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(8000, 64000)\n",
    "\n",
    "# Zweite Y-Achse für Naive Bayes (wegen der sehr kleinen Werte)\n",
    "ax2 = ax.twinx()\n",
    "df_nb_only = df_all[df_all['classifier'] == 'Naive Bayes']\n",
    "df_nb_grouped = df_nb_only.groupby('n_labeled')['avg_train_time'].agg(['mean']).reset_index()\n",
    "ax2.plot(df_nb_grouped['n_labeled'], df_nb_grouped['mean'], \n",
    "        color='red', marker='^', markersize=8, linewidth=2, \n",
    "        alpha=0.6, linestyle=':', label='Naive Bayes (rechte Achse)')\n",
    "ax2.set_ylabel('Trainingszeit Naive Bayes (Sekunden)', fontsize=11, color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot 6: Relative Performanz (normalisiert auf Random Sampling)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Relative Trainingszeit im Vergleich zur Zufallsauswahl', fontsize=16, y=1.02)\n",
    "\n",
    "for idx, classifier in enumerate(classifiers):\n",
    "    ax = axes.flatten()[idx]\n",
    "    df_classifier = df_all[df_all['classifier'] == classifier]\n",
    "    \n",
    "    # Random Sampling als Baseline\n",
    "    df_random = df_classifier[df_classifier['strategy_de'] == 'Zufallsauswahl']\n",
    "    baseline = df_random.groupby('n_labeled')['avg_train_time'].mean()\n",
    "    \n",
    "    for strategy in df_classifier['strategy_de'].unique():\n",
    "        if strategy != 'Zufallsauswahl':\n",
    "            df_strategy = df_classifier[df_classifier['strategy_de'] == strategy]\n",
    "            df_grouped = df_strategy.groupby('n_labeled')['avg_train_time'].mean()\n",
    "            \n",
    "            relative_time = (df_grouped / baseline) * 100\n",
    "            ax.plot(relative_time.index, relative_time.values, \n",
    "                   marker='o', label=strategy, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.axhline(y=100, color='black', linestyle='--', alpha=0.5, label='Zufallsauswahl (100%)')\n",
    "    ax.set_xlabel('Anzahl gelabelter Daten', fontsize=11)\n",
    "    ax.set_ylabel('Relative Trainingszeit (%)', fontsize=11)\n",
    "    ax.set_title(f'{classifier}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True, fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(8000, 64000)\n",
    "\n",
    "# Leeres Subplot entfernen\n",
    "axes.flatten()[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "pdf_pages.savefig(fig, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# PDF schließen\n",
    "pdf_pages.close()\n",
    "\n",
    "# Zusammenfassende Statistiken ausgeben\n",
    "print(\"=\" * 80)\n",
    "print(\"ZUSAMMENFASSUNG DER TRAININGSZEITEN\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nDurchschnittliche Trainingszeiten nach Klassifikator (in Sekunden):\")\n",
    "print(\"-\" * 60)\n",
    "summary_clf = df_all.groupby('classifier')['avg_train_time'].agg(['mean', 'std', 'min', 'max'])\n",
    "summary_clf = summary_clf.round(4)\n",
    "print(summary_clf.to_string())\n",
    "\n",
    "print(\"\\n\\nDurchschnittliche Trainingszeiten nach Query-Strategie (in Sekunden):\")\n",
    "print(\"-\" * 60)\n",
    "summary_strategy = df_all.groupby('strategy_de')['avg_train_time'].agg(['mean', 'std', 'min', 'max'])\n",
    "summary_strategy = summary_strategy.round(4)\n",
    "print(summary_strategy.to_string())\n",
    "\n",
    "print(\"\\n\\nSkalierungsfaktor (60k vs 12k Daten) nach Klassifikator:\")\n",
    "print(\"-\" * 60)\n",
    "for classifier in classifiers:\n",
    "    df_clf = df_all[df_all['classifier'] == classifier]\n",
    "    time_12k = df_clf[df_clf['n_labeled'] == 12000]['avg_train_time'].mean()\n",
    "    time_60k = df_clf[df_clf['n_labeled'] == 60000]['avg_train_time'].mean()\n",
    "    scaling_factor = time_60k / time_12k\n",
    "    print(f\"{classifier:20s}: {scaling_factor:.2f}x\")\n",
    "\n",
    "print(\"\\n\\nEffizienteste Kombination (Top 5 schnellste):\")\n",
    "print(\"-\" * 60)\n",
    "df_mean = df_all.groupby(['classifier', 'strategy_de', 'n_labeled'])['avg_train_time'].mean().reset_index()\n",
    "df_mean_sorted = df_mean.sort_values('avg_train_time').head(10)\n",
    "for idx, row in df_mean_sorted.iterrows():\n",
    "    print(f\"{row['classifier']:20s} | {row['strategy_de']:20s} | {row['n_labeled']:6.0f} Daten | {row['avg_train_time']:.4f} Sek\")\n",
    "\n",
    "print(\"\\n\\nLangsamste Kombination (Top 5 langsamste):\")\n",
    "print(\"-\" * 60)\n",
    "df_mean_sorted_slow = df_mean.sort_values('avg_train_time', ascending=False).head(10)\n",
    "for idx, row in df_mean_sorted_slow.iterrows():\n",
    "    print(f\"{row['classifier']:20s} | {row['strategy_de']:20s} | {row['n_labeled']:6.0f} Daten | {row['avg_train_time']:.4f} Sek\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PDF 'active_learning_trainingszeiten.pdf' wurde erfolgreich erstellt!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
