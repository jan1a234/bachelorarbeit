{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336ec0d-2afa-4177-9c8b-9d510018879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "=================================================================\n",
    "Aktives und Passives Lernen ohne modAL - Korrigierte Version\n",
    "=================================================================\n",
    "Professionelles Skript für MNIST-Experimente mit fairem Vergleich\n",
    "zwischen aktivem und passivem Lernen.\n",
    "\n",
    "Hauptkorrektur: Der aktive Lerner hat jetzt Zugriff auf den GESAMTEN\n",
    "Trainingspool und kann daraus die informativsten Samples auswählen,\n",
    "anstatt nur aus einer vorab zufällig ausgewählten Teilmenge.\n",
    "\n",
    "Wichtig: KEIN Warm-Start - CNN wird bei jedem Schritt neu initialisiert\n",
    "\n",
    "Metriken:\n",
    "- Anzahl gelabelter Instanzen\n",
    "- Accuracy, F1-Score (macro)\n",
    "- Trainingszeit pro Query-Zyklus\n",
    "- Auswahl-Diversität im flachen Feature-Space\n",
    "\n",
    "Classifiers:\n",
    "- CNN (PyTorch)\n",
    "- SVM, RandomForest, LogisticRegression, NaiveBayes (scikit-learn)\n",
    "\n",
    "Optimierungen:\n",
    "- Reproduzierbarkeit (Seed)\n",
    "- Batch-Mode bei Queries (50 Instanzen pro Zyklus)\n",
    "- Subsampling des Pools (max. 10k Trainingsbeispiele)\n",
    "- Effiziente Pool-Verwaltung mit Boolean-Masken\n",
    "- Fairer Vergleich: CNN wird bei jedem Schritt neu trainiert (kein Warm-Start)\n",
    "\n",
    "Version: 2.1 - Fehlerkorrektur und Verbesserungen (MNIST-Version)\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Reproduzierbarkeit\n",
    "# -------------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Logging konfigurieren\n",
    "# -------------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1) MNIST laden, Subsampling, Normalisierung, Split\n",
    "# -------------------------------------------------------------------------------\n",
    "def load_mnist_data(test_size=0.2, random_state=SEED, max_train=10000):\n",
    "    \"\"\"\n",
    "    Lädt MNIST, normalisiert (mean=0.1307, std=0.3081),\n",
    "    subsamplet bis max_train und splittet in Train/Val/Test.\n",
    "    \"\"\"\n",
    "    train_ds = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    test_ds = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    # (N, 1, 28, 28)\n",
    "    X_full = train_ds.data.unsqueeze(1).float() / 255.0\n",
    "    # Normalisieren mit Standard-Mittelwert/Std. für MNIST:\n",
    "    mean_val = 0.1307\n",
    "    std_val  = 0.3081\n",
    "    X_full = (X_full - mean_val) / std_val\n",
    "\n",
    "    y_full = train_ds.targets.numpy()\n",
    "\n",
    "    # Subsampling auf max_train\n",
    "    idx_sub = np.random.choice(len(X_full), size=min(max_train, len(X_full)), replace=False)\n",
    "    X_sub = X_full[idx_sub].numpy()\n",
    "    y_sub = y_full[idx_sub]\n",
    "\n",
    "    # Testdaten vorbereiten (auch hier normalisieren):\n",
    "    X_test = test_ds.data.unsqueeze(1).float() / 255.0\n",
    "    X_test = (X_test - mean_val) / std_val\n",
    "    y_test = test_ds.targets.numpy()\n",
    "\n",
    "    # Split in (Train/Val)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_sub, y_sub, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Daten geladen: Train={len(X_train)}, Val={len(X_val)}, Test={len(y_test)}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test.numpy(), y_test\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2) Einfaches CNN (PyTorch)\n",
    "# -------------------------------------------------------------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Einfache CNN-Architektur für MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.fc1   = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2   = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def fit_cnn(self, X_np, y_np, epochs=2, lr=1e-3, batch_size=128):\n",
    "        \"\"\"\n",
    "        Trainiert das CNN über 'epochs' mit Adam und CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        ds = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_np).float(),\n",
    "            torch.from_numpy(y_np).long()\n",
    "        )\n",
    "        loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(self(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X_np):\n",
    "        \"\"\"\n",
    "        Gibt die Modellwahrscheinlichkeiten zurück.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.from_numpy(X_np).float().to(self.device)\n",
    "            logits = self(X_t)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3) Klassische Klassifikatoren (Sklearn)\n",
    "# -------------------------------------------------------------------------------\n",
    "CLASSIFIERS = {\n",
    "    'SVM': partial(SVC, probability=True, kernel='rbf', random_state=SEED),\n",
    "    'RandomForest': partial(RandomForestClassifier, n_estimators=100, n_jobs=-1, random_state=SEED),\n",
    "    'LogisticRegression': partial(LogisticRegression, solver='lbfgs', max_iter=1000, random_state=SEED, n_jobs=-1),\n",
    "    'NaiveBayes': partial(GaussianNB),\n",
    "    'CNN': SimpleCNN\n",
    "}\n",
    "\n",
    "def train_and_predict(clf_key, X_tr, y_tr, X_te):\n",
    "    \"\"\"\n",
    "    Trainiert das gewünschte Modell (CNN oder Sklearn)\n",
    "    und gibt Predictions + Modell zurück.\n",
    "    \"\"\"\n",
    "    # CNN-Sonderfall\n",
    "    if clf_key == 'CNN':\n",
    "        # Immer ein neues CNN initialisieren für fairen Vergleich\n",
    "        cnn = SimpleCNN().fit_cnn(X_tr, y_tr, epochs=2)\n",
    "        preds = np.argmax(cnn.predict_proba(X_te), axis=1)\n",
    "        return preds, cnn\n",
    "    else:\n",
    "        # Klassischer Sklearn-Klassifikator\n",
    "        model = CLASSIFIERS[clf_key]()\n",
    "        # Flatten für Sklearn (28*28 = 784 Features)\n",
    "        model.fit(X_tr.reshape(len(X_tr), -1), y_tr)\n",
    "        preds = model.predict(X_te.reshape(len(X_te), -1))\n",
    "        return preds, model\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4) Query-/Uncertainty-Funktionen\n",
    "# -------------------------------------------------------------------------------\n",
    "def predict_proba(model, X):\n",
    "    \"\"\"\n",
    "    Vereinheitlichte Probability-Abfrage für CNN und Sklearn-Modelle.\n",
    "    \"\"\"\n",
    "    if isinstance(model, SimpleCNN):\n",
    "        return model.predict_proba(X)\n",
    "    else:\n",
    "        # Für Sklearn: Flatten auf (N, 28*28)\n",
    "        return model.predict_proba(X.reshape(len(X), -1))\n",
    "\n",
    "def least_confidence(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit geringster \"Confidence\" (max. Klasse).\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    scores = np.max(p, axis=1)  # highest predicted probability\n",
    "    return np.argsort(scores)[:k]\n",
    "\n",
    "def margin(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit kleinstem Margin zwischen den Top-2 Klassen.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    top2 = np.sort(p, axis=1)[:, -2:]\n",
    "    margins = top2[:, 1] - top2[:, 0]\n",
    "    return np.argsort(margins)[:k]\n",
    "\n",
    "def entropy_uncertainty(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit höchster Entropie aus.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    ent = (-p * np.log(p + 1e-9)).sum(axis=1)\n",
    "    return np.argsort(ent)[-k:]\n",
    "\n",
    "def information_density(model, X, k=1, subsample_size=1000):\n",
    "    \"\"\"\n",
    "    Mischt Entropie mit \"Density\" im Feature-Space.\n",
    "    Für Effizienz: Berechnet Density nur auf einem Subsample.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    ent = (-p * np.log(p + 1e-9)).sum(axis=1)\n",
    "\n",
    "    # Für große Pools: Subsample für Density-Berechnung\n",
    "    if len(X) > subsample_size:\n",
    "        sub_idx = np.random.choice(len(X), subsample_size, replace=False)\n",
    "        X_sub = X[sub_idx]\n",
    "    else:\n",
    "        X_sub = X\n",
    "        sub_idx = np.arange(len(X))\n",
    "\n",
    "    flat = X_sub.reshape(len(X_sub), -1)\n",
    "\n",
    "    # Berechne Durchschnittsdistanz zu k nächsten Nachbarn\n",
    "    k_neighbors = min(10, len(X_sub) - 1)\n",
    "    densities = np.zeros(len(X))\n",
    "\n",
    "    for i in range(len(X_sub)):\n",
    "        dists = np.sqrt(((flat[i] - flat) ** 2).sum(axis=1))\n",
    "        dists[i] = np.inf  # Exclude self\n",
    "        nearest = np.sort(dists)[:k_neighbors]\n",
    "        densities[sub_idx[i]] = 1.0 / (nearest.mean() + 1e-9)\n",
    "\n",
    "    # Für nicht-subsample Punkte: Durchschnittliche Density\n",
    "    if len(X) > subsample_size:\n",
    "        mean_density = densities[sub_idx].mean()\n",
    "        densities[densities == 0] = mean_density\n",
    "\n",
    "    scores = ent * densities\n",
    "    return np.argsort(scores)[-k:]\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5) Active-Learning-Schleife im Batch-Mode (KORRIGIERT)\n",
    "# -------------------------------------------------------------------------------\n",
    "def active_learning(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    strategy, clf_key, budget, runs, run_offset=0,\n",
    "                    batch_size=50):\n",
    "    \"\"\"\n",
    "    Führt Active Learning mit gegebener Strategie, Klassifikator und Budget durch.\n",
    "\n",
    "    WICHTIGE KORREKTUR: Der aktive Lerner hat jetzt Zugriff auf den GESAMTEN\n",
    "    Trainingspool, nicht nur auf eine vorab ausgewählte Teilmenge.\n",
    "\n",
    "    - batch_size: Anzahl Instanzen pro Query\n",
    "    - runs: Anzahl Wiederholungen für Statistik\n",
    "    - Budget: Anteil (0.0 - 1.0) an den verfügbaren Trainingsdaten\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(X_train)\n",
    "    n_label = int(budget * n)\n",
    "\n",
    "    if n_label < batch_size:\n",
    "        logger.warning(f\"Budget zu klein (n_label={n_label} < batch_size={batch_size})\")\n",
    "        return results\n",
    "\n",
    "    for run_i in range(runs):\n",
    "        run_id = run_offset + run_i\n",
    "        logger.info(f\"[{clf_key}][{strategy}] Run {run_i+1}/{runs} — Budget {budget:.1%} ({n_label} samples)\")\n",
    "\n",
    "        # ================================\n",
    "        # PASSIVES LERNEN (Baseline)\n",
    "        # ================================\n",
    "        idx_passive = np.random.choice(n, n_label, replace=False)\n",
    "\n",
    "        # Messen der Train-Zeit im passiven Modus\n",
    "        t0 = time.time()\n",
    "        y_pred_passive, _ = train_and_predict(\n",
    "            clf_key,\n",
    "            X_train[idx_passive],\n",
    "            y_train[idx_passive],\n",
    "            X_test\n",
    "        )\n",
    "        pass_train_time = time.time() - t0\n",
    "\n",
    "        # Ergebnisse (passiv)\n",
    "        acc_passive = accuracy_score(y_test, y_pred_passive)\n",
    "        f1_passive = f1_score(y_test, y_pred_passive, average='macro')\n",
    "\n",
    "        results.append([\n",
    "            \"passiv\",\n",
    "            clf_key,\n",
    "            strategy,\n",
    "            budget,\n",
    "            run_id,\n",
    "            \"final\",\n",
    "            n_label,\n",
    "            acc_passive,\n",
    "            f1_passive,\n",
    "            pass_train_time,\n",
    "            0.0  # diversity\n",
    "        ])\n",
    "\n",
    "        logger.info(f\"  Passiv: {n_label} labels → Acc={acc_passive:.3f}, F1={f1_passive:.3f}\")\n",
    "\n",
    "        # ================================\n",
    "        # AKTIVES LERNEN (KORRIGIERT)\n",
    "        # ================================\n",
    "        # KRITISCHE ÄNDERUNG: Der Pool ist das GESAMTE Trainingsset\n",
    "        X_pool = X_train.copy()\n",
    "        y_pool = y_train.copy()\n",
    "\n",
    "        # Tracking mit Boolean-Maske (effizienter als np.delete)\n",
    "        is_labeled = np.zeros(n, dtype=bool)\n",
    "\n",
    "        # Gelabelte Daten sammeln\n",
    "        labeled_indices = []\n",
    "\n",
    "        q_steps = int(np.ceil(n_label / batch_size))\n",
    "\n",
    "        for q in range(q_steps):\n",
    "            # Anzahl zu labelender Samples in diesem Schritt\n",
    "            remaining_budget = n_label - len(labeled_indices)\n",
    "            b_size_current = min(batch_size, remaining_budget)\n",
    "\n",
    "            if b_size_current <= 0:\n",
    "                break\n",
    "\n",
    "            # Unlabeled pool indices\n",
    "            unlabeled_mask = ~is_labeled\n",
    "            unlabeled_indices = np.where(unlabeled_mask)[0]\n",
    "\n",
    "            # Modell trainieren, falls schon gelabelte Daten vorliegen\n",
    "            if len(labeled_indices) > 0:\n",
    "                t0 = time.time()\n",
    "                X_lab = X_train[labeled_indices]\n",
    "                y_lab = y_train[labeled_indices]\n",
    "                # KORREKTUR: Kein warm_cnn Parameter\n",
    "                _, model = train_and_predict(clf_key, X_lab, y_lab, X_test)\n",
    "                train_time = time.time() - t0\n",
    "            else:\n",
    "                train_time = 0.0\n",
    "                model = None\n",
    "\n",
    "            # Auswahl via aktiver Strategie\n",
    "            if model is None or len(labeled_indices) < 10:  # Bootstrap mit Random für erste Samples\n",
    "                # Zufällige Auswahl für initiale Samples\n",
    "                selected_pool_idx = np.random.choice(\n",
    "                    len(unlabeled_indices),\n",
    "                    size=min(b_size_current, len(unlabeled_indices)),\n",
    "                    replace=False\n",
    "                )\n",
    "            else:\n",
    "                # Aktive Auswahl aus dem unlabeled Pool\n",
    "                X_unlabeled = X_train[unlabeled_indices]\n",
    "\n",
    "                if strategy == 'least_confidence':\n",
    "                    selected_pool_idx = least_confidence(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'margin':\n",
    "                    selected_pool_idx = margin(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'entropy':\n",
    "                    selected_pool_idx = entropy_uncertainty(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'information_density':\n",
    "                    selected_pool_idx = information_density(model, X_unlabeled, k=b_size_current)\n",
    "                else:\n",
    "                    selected_pool_idx = entropy_uncertainty(model, X_unlabeled, k=b_size_current)\n",
    "\n",
    "            # Konvertiere Pool-Indizes zu globalen Indizes\n",
    "            selected_global_idx = unlabeled_indices[selected_pool_idx]\n",
    "\n",
    "            # Diversität der ausgewählten Samples berechnen\n",
    "            if len(selected_global_idx) > 1:\n",
    "                feats = X_train[selected_global_idx].reshape(len(selected_global_idx), -1)\n",
    "                pairwise = list(combinations(feats, 2))\n",
    "                diversity = np.mean([np.linalg.norm(a - b) for a, b in pairwise]) if pairwise else 0.0\n",
    "            else:\n",
    "                diversity = 0.0\n",
    "\n",
    "            # Update labeled indices und mask\n",
    "            labeled_indices.extend(selected_global_idx.tolist())\n",
    "            is_labeled[selected_global_idx] = True\n",
    "\n",
    "            # Test-Evaluation\n",
    "            if len(labeled_indices) > 0:\n",
    "                X_lab = X_train[labeled_indices]\n",
    "                y_lab = y_train[labeled_indices]\n",
    "                y_eval, _ = train_and_predict(clf_key, X_lab, y_lab, X_test)\n",
    "                acc = accuracy_score(y_test, y_eval)\n",
    "                f1  = f1_score(y_test, y_eval, average='macro')\n",
    "            else:\n",
    "                acc = 0.0\n",
    "                f1 = 0.0\n",
    "\n",
    "            # Schritt-Ergebnis loggen\n",
    "            results.append([\n",
    "                \"aktiv\",\n",
    "                clf_key,\n",
    "                strategy,\n",
    "                budget,\n",
    "                run_id,\n",
    "                q,\n",
    "                len(labeled_indices),\n",
    "                acc,\n",
    "                f1,\n",
    "                train_time,\n",
    "                diversity\n",
    "            ])\n",
    "\n",
    "            # Jeden Schritt loggen für bessere Transparenz\n",
    "            logger.info(f\"  Aktiv [{strategy}] Schritt {q+1}/{q_steps}: \"\n",
    "                      f\"{len(labeled_indices)} labels → Acc={acc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 6) Evaluation Utilities\n",
    "# -------------------------------------------------------------------------------\n",
    "def print_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Gibt eine übersichtliche Zusammenfassung der Ergebnisse aus.\n",
    "    \"\"\"\n",
    "    # Finale Ergebnisse (letzte Iteration jedes Runs)\n",
    "    final_results = df.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget', 'run_id']).last().reset_index()\n",
    "\n",
    "    # Aggregierte Statistiken\n",
    "    summary = final_results.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'f1_macro': ['mean', 'std'],\n",
    "        'train_time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ZUSAMMENFASSUNG DER ERGEBNISSE\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary)\n",
    "\n",
    "    # Vergleich Aktiv vs Passiv pro Budget\n",
    "    for budget in sorted(df['budget'].unique()):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BUDGET: {budget:.1%}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        budget_data = final_results[final_results['budget'] == budget]\n",
    "\n",
    "        for clf in sorted(df['klassifizierer'].unique()):\n",
    "            clf_data = budget_data[budget_data['klassifizierer'] == clf]\n",
    "            passive_clf = clf_data[clf_data['lernmodus'] == 'passiv']['accuracy']\n",
    "\n",
    "            if passive_clf.empty:\n",
    "                continue\n",
    "\n",
    "            passive_mean = passive_clf.mean()\n",
    "            passive_std = passive_clf.std()\n",
    "\n",
    "            print(f\"\\n{clf}:\")\n",
    "            print(f\"  Passiv: {passive_mean:.3f} ± {passive_std:.3f}\")\n",
    "\n",
    "            improvements = []\n",
    "            for strategy in sorted(df['strategie'].unique()):\n",
    "                active_data = clf_data[(clf_data['lernmodus'] == 'aktiv') &\n",
    "                                      (clf_data['strategie'] == strategy)]\n",
    "                if not active_data.empty:\n",
    "                    active_acc = active_data['accuracy']\n",
    "                    active_mean = active_acc.mean()\n",
    "                    active_std = active_acc.std()\n",
    "                    improvement = (active_mean - passive_mean) * 100\n",
    "                    improvements.append((strategy, active_mean, active_std, improvement))\n",
    "                    print(f\"  Aktiv ({strategy:20}): {active_mean:.3f} ± {active_std:.3f} ({improvement:+.1f}%)\")\n",
    "\n",
    "            # Beste Strategie markieren\n",
    "            if improvements:\n",
    "                best_strategy = max(improvements, key=lambda x: x[1])\n",
    "                print(f\"  → Beste Strategie: {best_strategy[0]} mit {best_strategy[3]:+.1f}% Verbesserung\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Hauptprogramm\n",
    "# -------------------------------------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Haupt-Einstiegspunkt:\n",
    "    1) MNIST-Daten laden\n",
    "    2) Budget- und Strategie-Settings definieren\n",
    "    3) Experiment-Schleife\n",
    "    4) Ergebnisse speichern und zusammenfassen\n",
    "    \"\"\"\n",
    "    # 1) MNIST-Daten laden\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mnist_data(\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        max_train=10000  # Kann erhöht werden für umfangreichere Experimente\n",
    "    )\n",
    "\n",
    "    # 2) Mehrere Budget-Stufen definieren\n",
    "    budgets = [0.3, 0.6, 0.9]  # 30%, 60%, 90% des Trainingssets\n",
    "\n",
    "    # Mögliche AL-Strategien und Klassifikatoren\n",
    "    strategies = [\"least_confidence\", \"margin\", \"entropy\", \"information_density\"]\n",
    "    clf_keys   = [\"CNN\", \"SVM\", \"RandomForest\", \"LogisticRegression\", \"NaiveBayes\"]\n",
    "\n",
    "    # Anzahl Wiederholungen pro Setting\n",
    "    runs = 5\n",
    "\n",
    "    # 3) Experiment-Schleife\n",
    "    all_results = []\n",
    "    total_experiments = len(strategies) * len(clf_keys) * len(budgets)\n",
    "    exp_count = 0\n",
    "\n",
    "    for budget in budgets:\n",
    "        for clf_key in clf_keys:\n",
    "            for strategy in strategies:\n",
    "                exp_count += 1\n",
    "                print(f\"\\n[{exp_count}/{total_experiments}] \"\n",
    "                      f\"Budget={budget:.1%}, Classifier={clf_key}, Strategy={strategy}\")\n",
    "\n",
    "                res = active_learning(\n",
    "                    X_train, y_train,\n",
    "                    X_val,   y_val,\n",
    "                    X_test,  y_test,\n",
    "                    strategy=strategy,\n",
    "                    clf_key=clf_key,\n",
    "                    budget=budget,\n",
    "                    runs=runs,\n",
    "                    batch_size=50\n",
    "                )\n",
    "                all_results.extend(res)\n",
    "\n",
    "    # 4) DataFrame bauen und als CSV ablegen\n",
    "    df = pd.DataFrame(\n",
    "        all_results,\n",
    "        columns=[\n",
    "            \"lernmodus\",      # \"passiv\" oder \"aktiv\"\n",
    "            \"klassifizierer\", # \"CNN\", \"SVM\", ...\n",
    "            \"strategie\",      # \"least_confidence\", ...\n",
    "            \"budget\",         # 0.3, 0.6, 0.9, ...\n",
    "            \"run_id\",\n",
    "            \"zyklus\",\n",
    "            \"anz_label\",\n",
    "            \"accuracy\",\n",
    "            \"f1_macro\",\n",
    "            \"train_time\",\n",
    "            \"diversity\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    output_file = \"ergebnisse_mnist_corrected.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Ergebnisse in '{output_file}' gespeichert.\")\n",
    "\n",
    "    # Zusammenfassung ausgeben\n",
    "    print_summary_statistics(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
