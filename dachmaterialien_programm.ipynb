{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2a2f2-8da6-43a8-ae8d-cd3df9e872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "=================================================================\n",
    "Aktives und Passives Lernen für Dachmaterial-Klassifikation\n",
    "=================================================================\n",
    "Professionelles Skript für Dachmaterial-Experimente mit fairem Vergleich\n",
    "zwischen aktivem und passivem Lernen.\n",
    "\n",
    "Basiert auf dem MNIST Active Learning Framework, angepasst für\n",
    "tabellarische Daten mit kategorischen und numerischen Features.\n",
    "\n",
    "Dataset: umrisse_with_all_data_and_shape_and_patch_and_normal.csv\n",
    "Zielvariable: mat_qgis (11 Klassen von Dachmaterialien)\n",
    "\n",
    "Features:\n",
    "- area: Numerisch (Fläche)\n",
    "- area_type: Kategorisch\n",
    "- Shape: Kategorisch\n",
    "- ezg: Kategorisch\n",
    "\n",
    "Classifiers:\n",
    "- TabularNN (Neuronales Netz für tabellarische Daten)\n",
    "- SVM, RandomForest, LogisticRegression, NaiveBayes\n",
    "\n",
    "Strategien:\n",
    "- least_confidence\n",
    "- margin\n",
    "- entropy\n",
    "- information_density\n",
    "\n",
    "Version: 1.0 - Dachmaterial-Version\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Reproduzierbarkeit\n",
    "# -------------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Logging konfigurieren\n",
    "# -------------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1) Daten laden und vorbereiten\n",
    "# -------------------------------------------------------------------------------\n",
    "def load_dachmaterial_data(filepath, target_col='mat_qgis', test_size=0.2, \n",
    "                          val_size=0.2, random_state=SEED, max_train=10000):\n",
    "    \"\"\"\n",
    "    Lädt den Dachmaterial-Datensatz und bereitet ihn für ML vor.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Pfad zur CSV-Datei\n",
    "        target_col: Zielvariable (Standard: 'mat_qgis')\n",
    "        test_size: Anteil der Testdaten\n",
    "        val_size: Anteil der Validierungsdaten vom Trainingsset\n",
    "        random_state: Random Seed\n",
    "        max_train: Maximale Anzahl Trainingsbeispiele\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed data splits und zusätzliche Informationen\n",
    "    \"\"\"\n",
    "    # Daten laden\n",
    "    df = pd.read_csv(filepath)\n",
    "    logger.info(f\"Datensatz geladen: {len(df)} Zeilen, {len(df.columns)} Spalten\")\n",
    "    \n",
    "    # Klassen-Verteilung anzeigen\n",
    "    class_dist = df[target_col].value_counts()\n",
    "    logger.info(f\"Klassen-Verteilung:\\n{class_dist}\")\n",
    "    \n",
    "    # Feature-Spalten definieren\n",
    "    feature_cols = ['area', 'area_type', 'Shape', 'ezg']\n",
    "    \n",
    "    # Überprüfe ob alle Features vorhanden sind\n",
    "    missing_cols = [col for col in feature_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.warning(f\"Fehlende Spalten: {missing_cols}\")\n",
    "        feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    # Datentypen analysieren\n",
    "    logger.info(\"Analysiere Datentypen...\")\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            n_unique = df[col].nunique()\n",
    "            n_missing = df[col].isna().sum()\n",
    "            logger.info(f\"  {col}: dtype={dtype}, unique={n_unique}, missing={n_missing}\")\n",
    "            \n",
    "            # Bestimme ob Feature numerisch oder kategorisch ist\n",
    "            if col == 'area':  # area ist definitiv numerisch\n",
    "                numeric_features.append(col)\n",
    "            elif 'type' in col.lower() or col in ['Shape', 'ezg']:\n",
    "                categorical_features.append(col)\n",
    "            elif df[col].dtype == 'object':\n",
    "                categorical_features.append(col)\n",
    "            elif df[col].nunique() < 20:\n",
    "                categorical_features.append(col)\n",
    "            else:\n",
    "                numeric_features.append(col)\n",
    "    \n",
    "    logger.info(f\"Numerische Features: {numeric_features}\")\n",
    "    logger.info(f\"Kategoriale Features: {categorical_features}\")\n",
    "    \n",
    "    # Nur Zeilen mit gültiger Zielvariable behalten\n",
    "    df = df[df[target_col].notna()].copy()\n",
    "    \n",
    "    # Features und Target trennen\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Label Encoding für Target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    logger.info(f\"Anzahl Klassen: {n_classes}\")\n",
    "    logger.info(f\"Klassen: {list(label_encoder.classes_)}\")\n",
    "    \n",
    "    # Preprocessing Pipeline erstellen\n",
    "    # Numerische Features: Imputing + Scaling\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Kategorische Features: Imputing + One-Hot Encoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # ColumnTransformer kombiniert beide\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Stratified Split für ausgeglichene Klassen-Verteilung\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, random_state=random_state, \n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Subsampling falls gewünscht\n",
    "    if len(X_temp) > max_train:\n",
    "        # Stratified sampling für Subsampling\n",
    "        _, X_temp, _, y_temp = train_test_split(\n",
    "            X_temp, y_temp, test_size=max_train/len(X_temp), \n",
    "            random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    \n",
    "    # Train/Val Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Preprocessor fitten und transformieren\n",
    "    X_train_processed = preprocessor.fit_transform(X_train).astype(np.float32)\n",
    "    X_val_processed = preprocessor.transform(X_val).astype(np.float32)\n",
    "    X_test_processed = preprocessor.transform(X_test).astype(np.float32)\n",
    "    \n",
    "    # Feature-Namen für spätere Verwendung extrahieren\n",
    "    feature_names = []\n",
    "    if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "        feature_names = list(preprocessor.get_feature_names_out())\n",
    "    else:\n",
    "        # Fallback für ältere sklearn Versionen\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'num':\n",
    "                feature_names.extend(features)\n",
    "            elif name == 'cat' and hasattr(transformer.named_steps['onehot'], 'get_feature_names_out'):\n",
    "                cat_features = transformer.named_steps['onehot'].get_feature_names_out(features)\n",
    "                feature_names.extend(cat_features)\n",
    "    \n",
    "    logger.info(f\"Daten vorbereitet: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    logger.info(f\"Feature-Dimension nach Preprocessing: {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    return (X_train_processed, y_train, \n",
    "            X_val_processed, y_val,\n",
    "            X_test_processed, y_test,\n",
    "            preprocessor, label_encoder, feature_names, n_classes)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2) Neuronales Netz für tabellarische Daten\n",
    "# -------------------------------------------------------------------------------\n",
    "class TabularNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neuronales Netz optimiert für tabellarische Daten mit kategorischen und\n",
    "    numerischen Features. Verwendet Batch Normalization und Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_classes, hidden_dims=[128, 64, 32], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], n_classes)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        logger.info(f\"TabularNN initialisiert auf {self.device}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (layer, bn, dropout) in enumerate(zip(self.layers, self.batch_norms, self.dropouts)):\n",
    "            x = layer(x)\n",
    "            if x.shape[0] > 1:  # Batch normalization benötigt batch_size > 1\n",
    "                x = bn(x)\n",
    "            x = self.activation(x)\n",
    "            x = dropout(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def fit_nn(self, X_np, y_np, X_val=None, y_val=None, epochs=50, lr=1e-3, \n",
    "               batch_size=128, patience=10):\n",
    "        \"\"\"\n",
    "        Trainiert das Neuronale Netz mit Early Stopping.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Dataset und DataLoader\n",
    "        dataset = TensorDataset(\n",
    "            torch.from_numpy(X_np).float(),\n",
    "            torch.from_numpy(y_np).long()\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Early Stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self(xb)\n",
    "                loss = loss_fn(output, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Validierung\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_val_t = torch.from_numpy(X_val).float().to(self.device)\n",
    "                    y_val_t = torch.from_numpy(y_val).long().to(self.device)\n",
    "                    val_output = self(X_val_t)\n",
    "                    val_loss = loss_fn(val_output, y_val_t).item()\n",
    "                \n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Early Stopping Check\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_state = self.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        if best_state is not None:\n",
    "                            self.load_state_dict(best_state)\n",
    "                        break\n",
    "                \n",
    "                self.train()\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X_np):\n",
    "        \"\"\"\n",
    "        Gibt die Klassenwahrscheinlichkeiten zurück.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.from_numpy(X_np).float().to(self.device)\n",
    "            logits = self(X_t)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3) Klassische Klassifikatoren\n",
    "# -------------------------------------------------------------------------------\n",
    "CLASSIFIERS = {\n",
    "    'SVM': partial(SVC, probability=True, kernel='rbf', gamma='scale', random_state=SEED),\n",
    "    'RandomForest': partial(RandomForestClassifier, n_estimators=100, max_depth=10, \n",
    "                           min_samples_split=5, n_jobs=-1, random_state=SEED),\n",
    "    'LogisticRegression': partial(LogisticRegression, solver='lbfgs', max_iter=1000, \n",
    "                                 random_state=SEED, n_jobs=-1),\n",
    "    'NaiveBayes': partial(GaussianNB),\n",
    "    'TabularNN': None  # Wird separat behandelt\n",
    "}\n",
    "\n",
    "def train_and_predict(clf_key, X_tr, y_tr, X_te, X_val=None, y_val=None, n_classes=None):\n",
    "    \"\"\"\n",
    "    Trainiert das gewünschte Modell und gibt Predictions + Modell zurück.\n",
    "    \"\"\"\n",
    "    if clf_key == 'TabularNN':\n",
    "        if n_classes is None:\n",
    "            n_classes = len(np.unique(y_tr))\n",
    "        input_dim = X_tr.shape[1]\n",
    "        nn = TabularNN(input_dim=input_dim, n_classes=n_classes)\n",
    "        nn.fit_nn(X_tr, y_tr, X_val, y_val, epochs=50, patience=10)\n",
    "        preds = np.argmax(nn.predict_proba(X_te), axis=1)\n",
    "        return preds, nn\n",
    "    else:\n",
    "        model = CLASSIFIERS[clf_key]()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        return preds, model\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4) Query-/Uncertainty-Funktionen (identisch mit MNIST)\n",
    "# -------------------------------------------------------------------------------\n",
    "def predict_proba(model, X):\n",
    "    \"\"\"\n",
    "    Vereinheitlichte Probability-Abfrage für NN und Sklearn-Modelle.\n",
    "    \"\"\"\n",
    "    if isinstance(model, TabularNN):\n",
    "        return model.predict_proba(X)\n",
    "    else:\n",
    "        return model.predict_proba(X)\n",
    "\n",
    "def least_confidence(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit geringster \"Confidence\" (max. Klasse).\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    scores = np.max(p, axis=1)\n",
    "    return np.argsort(scores)[:k]\n",
    "\n",
    "def margin(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit kleinstem Margin zwischen den Top-2 Klassen.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    top2 = np.sort(p, axis=1)[:, -2:]\n",
    "    margins = top2[:, 1] - top2[:, 0]\n",
    "    return np.argsort(margins)[:k]\n",
    "\n",
    "def entropy_uncertainty(model, X, k=1):\n",
    "    \"\"\"\n",
    "    Wählt k Samples mit höchster Entropie aus.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    ent = (-p * np.log(p + 1e-9)).sum(axis=1)\n",
    "    return np.argsort(ent)[-k:]\n",
    "\n",
    "def information_density(model, X, k=1, subsample_size=1000):\n",
    "    \"\"\"\n",
    "    Mischt Entropie mit \"Density\" im Feature-Space.\n",
    "    \"\"\"\n",
    "    p = predict_proba(model, X)\n",
    "    ent = (-p * np.log(p + 1e-9)).sum(axis=1)\n",
    "\n",
    "    # Für große Pools: Subsample für Density-Berechnung\n",
    "    if len(X) > subsample_size:\n",
    "        sub_idx = np.random.choice(len(X), subsample_size, replace=False)\n",
    "        X_sub = X[sub_idx]\n",
    "    else:\n",
    "        X_sub = X\n",
    "        sub_idx = np.arange(len(X))\n",
    "\n",
    "    # Berechne Durchschnittsdistanz zu k nächsten Nachbarn\n",
    "    k_neighbors = min(10, len(X_sub) - 1)\n",
    "    densities = np.zeros(len(X))\n",
    "\n",
    "    for i in range(len(X_sub)):\n",
    "        dists = np.sqrt(((X_sub[i] - X_sub) ** 2).sum(axis=1))\n",
    "        dists[i] = np.inf  # Exclude self\n",
    "        nearest = np.sort(dists)[:k_neighbors]\n",
    "        densities[sub_idx[i]] = 1.0 / (nearest.mean() + 1e-9)\n",
    "\n",
    "    # Für nicht-subsample Punkte: Durchschnittliche Density\n",
    "    if len(X) > subsample_size:\n",
    "        mean_density = densities[sub_idx].mean()\n",
    "        densities[densities == 0] = mean_density\n",
    "\n",
    "    scores = ent * densities\n",
    "    return np.argsort(scores)[-k:]\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5) Active-Learning-Schleife (angepasst für Dachmaterial)\n",
    "# -------------------------------------------------------------------------------\n",
    "def active_learning(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                    strategy, clf_key, budget, runs, n_classes,\n",
    "                    run_offset=0, batch_size=50):\n",
    "    \"\"\"\n",
    "    Führt Active Learning mit gegebener Strategie, Klassifikator und Budget durch.\n",
    "    \n",
    "    WICHTIG: Der aktive Lerner hat Zugriff auf den GESAMTEN Trainingspool.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(X_train)\n",
    "    n_label = int(budget * n)\n",
    "\n",
    "    if n_label < batch_size:\n",
    "        logger.warning(f\"Budget zu klein (n_label={n_label} < batch_size={batch_size})\")\n",
    "        return results\n",
    "\n",
    "    for run_i in range(runs):\n",
    "        run_id = run_offset + run_i\n",
    "        logger.info(f\"[{clf_key}][{strategy}] Run {run_i+1}/{runs} — Budget {budget:.1%} ({n_label} samples)\")\n",
    "\n",
    "        # ================================\n",
    "        # PASSIVES LERNEN (Baseline)\n",
    "        # ================================\n",
    "        idx_passive = np.random.choice(n, n_label, replace=False)\n",
    "\n",
    "        t0 = time.time()\n",
    "        y_pred_passive, _ = train_and_predict(\n",
    "            clf_key,\n",
    "            X_train[idx_passive],\n",
    "            y_train[idx_passive],\n",
    "            X_test,\n",
    "            X_val, y_val,\n",
    "            n_classes\n",
    "        )\n",
    "        pass_train_time = time.time() - t0\n",
    "\n",
    "        # Ergebnisse (passiv)\n",
    "        acc_passive = accuracy_score(y_test, y_pred_passive)\n",
    "        f1_passive = f1_score(y_test, y_pred_passive, average='macro')\n",
    "\n",
    "        results.append([\n",
    "            \"passiv\",\n",
    "            clf_key,\n",
    "            strategy,\n",
    "            budget,\n",
    "            run_id,\n",
    "            \"final\",\n",
    "            n_label,\n",
    "            acc_passive,\n",
    "            f1_passive,\n",
    "            pass_train_time,\n",
    "            0.0  # diversity\n",
    "        ])\n",
    "\n",
    "        logger.info(f\"  Passiv: {n_label} labels → Acc={acc_passive:.3f}, F1={f1_passive:.3f}\")\n",
    "\n",
    "        # ================================\n",
    "        # AKTIVES LERNEN\n",
    "        # ================================\n",
    "        # Der Pool ist das GESAMTE Trainingsset\n",
    "        is_labeled = np.zeros(n, dtype=bool)\n",
    "        labeled_indices = []\n",
    "\n",
    "        q_steps = int(np.ceil(n_label / batch_size))\n",
    "\n",
    "        for q in range(q_steps):\n",
    "            # Anzahl zu labelender Samples in diesem Schritt\n",
    "            remaining_budget = n_label - len(labeled_indices)\n",
    "            b_size_current = min(batch_size, remaining_budget)\n",
    "\n",
    "            if b_size_current <= 0:\n",
    "                break\n",
    "\n",
    "            # Unlabeled pool indices\n",
    "            unlabeled_mask = ~is_labeled\n",
    "            unlabeled_indices = np.where(unlabeled_mask)[0]\n",
    "\n",
    "            # Modell trainieren, falls schon gelabelte Daten vorliegen\n",
    "            if len(labeled_indices) > 0:\n",
    "                t0 = time.time()\n",
    "                X_lab = X_train[labeled_indices]\n",
    "                y_lab = y_train[labeled_indices]\n",
    "                _, model = train_and_predict(clf_key, X_lab, y_lab, X_test, X_val, y_val, n_classes)\n",
    "                train_time = time.time() - t0\n",
    "            else:\n",
    "                train_time = 0.0\n",
    "                model = None\n",
    "\n",
    "            # Auswahl via aktiver Strategie\n",
    "            if model is None or len(labeled_indices) < 10:  # Bootstrap mit Random\n",
    "                selected_pool_idx = np.random.choice(\n",
    "                    len(unlabeled_indices),\n",
    "                    size=min(b_size_current, len(unlabeled_indices)),\n",
    "                    replace=False\n",
    "                )\n",
    "            else:\n",
    "                # Aktive Auswahl aus dem unlabeled Pool\n",
    "                X_unlabeled = X_train[unlabeled_indices]\n",
    "\n",
    "                if strategy == 'least_confidence':\n",
    "                    selected_pool_idx = least_confidence(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'margin':\n",
    "                    selected_pool_idx = margin(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'entropy':\n",
    "                    selected_pool_idx = entropy_uncertainty(model, X_unlabeled, k=b_size_current)\n",
    "                elif strategy == 'information_density':\n",
    "                    selected_pool_idx = information_density(model, X_unlabeled, k=b_size_current)\n",
    "                else:\n",
    "                    selected_pool_idx = entropy_uncertainty(model, X_unlabeled, k=b_size_current)\n",
    "\n",
    "            # Konvertiere Pool-Indizes zu globalen Indizes\n",
    "            selected_global_idx = unlabeled_indices[selected_pool_idx]\n",
    "\n",
    "            # Diversität der ausgewählten Samples berechnen\n",
    "            if len(selected_global_idx) > 1:\n",
    "                feats = X_train[selected_global_idx]\n",
    "                pairwise = list(combinations(feats, 2))\n",
    "                diversity = np.mean([np.linalg.norm(a - b) for a, b in pairwise]) if pairwise else 0.0\n",
    "            else:\n",
    "                diversity = 0.0\n",
    "\n",
    "            # Update labeled indices und mask\n",
    "            labeled_indices.extend(selected_global_idx.tolist())\n",
    "            is_labeled[selected_global_idx] = True\n",
    "\n",
    "            # Test-Evaluation\n",
    "            if len(labeled_indices) > 0:\n",
    "                X_lab = X_train[labeled_indices]\n",
    "                y_lab = y_train[labeled_indices]\n",
    "                y_eval, _ = train_and_predict(clf_key, X_lab, y_lab, X_test, X_val, y_val, n_classes)\n",
    "                acc = accuracy_score(y_test, y_eval)\n",
    "                f1  = f1_score(y_test, y_eval, average='macro')\n",
    "            else:\n",
    "                acc = 0.0\n",
    "                f1 = 0.0\n",
    "\n",
    "            # Schritt-Ergebnis loggen\n",
    "            results.append([\n",
    "                \"aktiv\",\n",
    "                clf_key,\n",
    "                strategy,\n",
    "                budget,\n",
    "                run_id,\n",
    "                q,\n",
    "                len(labeled_indices),\n",
    "                acc,\n",
    "                f1,\n",
    "                train_time,\n",
    "                diversity\n",
    "            ])\n",
    "\n",
    "            # Jeden 5. Schritt loggen\n",
    "            if q % 5 == 0:\n",
    "                logger.info(f\"  Aktiv [{strategy}] Schritt {q+1}/{q_steps}: \"\n",
    "                          f\"{len(labeled_indices)} labels → Acc={acc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 6) Evaluation Utilities\n",
    "# -------------------------------------------------------------------------------\n",
    "def print_summary_statistics(df, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Gibt eine übersichtliche Zusammenfassung der Ergebnisse aus.\n",
    "    \"\"\"\n",
    "    # Finale Ergebnisse (letzte Iteration jedes Runs)\n",
    "    final_results = df.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget', 'run_id']).last().reset_index()\n",
    "\n",
    "    # Aggregierte Statistiken\n",
    "    summary = final_results.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'f1_macro': ['mean', 'std'],\n",
    "        'train_time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ZUSAMMENFASSUNG DER ERGEBNISSE\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary)\n",
    "\n",
    "    # Vergleich Aktiv vs Passiv pro Budget\n",
    "    for budget in sorted(df['budget'].unique()):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BUDGET: {budget:.1%}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        budget_data = final_results[final_results['budget'] == budget]\n",
    "\n",
    "        for clf in sorted(df['klassifizierer'].unique()):\n",
    "            clf_data = budget_data[budget_data['klassifizierer'] == clf]\n",
    "            passive_clf = clf_data[clf_data['lernmodus'] == 'passiv']['accuracy']\n",
    "\n",
    "            if passive_clf.empty:\n",
    "                continue\n",
    "\n",
    "            passive_mean = passive_clf.mean()\n",
    "            passive_std = passive_clf.std()\n",
    "\n",
    "            print(f\"\\n{clf}:\")\n",
    "            print(f\"  Passiv: {passive_mean:.3f} ± {passive_std:.3f}\")\n",
    "\n",
    "            improvements = []\n",
    "            for strategy in sorted(df['strategie'].unique()):\n",
    "                active_data = clf_data[(clf_data['lernmodus'] == 'aktiv') &\n",
    "                                      (clf_data['strategie'] == strategy)]\n",
    "                if not active_data.empty:\n",
    "                    active_acc = active_data['accuracy']\n",
    "                    active_mean = active_acc.mean()\n",
    "                    active_std = active_acc.std()\n",
    "                    improvement = (active_mean - passive_mean) * 100\n",
    "                    improvements.append((strategy, active_mean, active_std, improvement))\n",
    "                    print(f\"  Aktiv ({strategy:20}): {active_mean:.3f} ± {active_std:.3f} ({improvement:+.1f}%)\")\n",
    "\n",
    "            # Beste Strategie markieren\n",
    "            if improvements:\n",
    "                best_strategy = max(improvements, key=lambda x: x[1])\n",
    "                print(f\"  → Beste Strategie: {best_strategy[0]} mit {best_strategy[3]:+.1f}% Verbesserung\")\n",
    "\n",
    "def create_visualizations(df, label_encoder=None, output_dir='plots_dachmaterial'):\n",
    "    \"\"\"\n",
    "    Erstellt Visualisierungen der Ergebnisse.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Style setzen\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    colors = sns.color_palette(\"husl\", n_colors=len(df['strategie'].unique()))\n",
    "    \n",
    "    # 1. Learning Curves pro Budget\n",
    "    for budget in sorted(df['budget'].unique()):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        budget_data = df[df['budget'] == budget]\n",
    "        \n",
    "        for idx, clf in enumerate(sorted(df['klassifizierer'].unique())):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            clf_data = budget_data[budget_data['klassifizierer'] == clf]\n",
    "            \n",
    "            # Passive baseline\n",
    "            passive_data = clf_data[clf_data['lernmodus'] == 'passiv']\n",
    "            if not passive_data.empty:\n",
    "                passive_acc = passive_data['accuracy'].mean()\n",
    "                ax.axhline(y=passive_acc, color='black', linestyle='--', \n",
    "                          label='Passiv', alpha=0.7, linewidth=2)\n",
    "            \n",
    "            # Active learning curves\n",
    "            for i, strategy in enumerate(sorted(df['strategie'].unique())):\n",
    "                strategy_data = clf_data[(clf_data['lernmodus'] == 'aktiv') &\n",
    "                                       (clf_data['strategie'] == strategy)]\n",
    "                \n",
    "                if not strategy_data.empty:\n",
    "                    grouped = strategy_data.groupby('anz_label')['accuracy'].agg(['mean', 'std'])\n",
    "                    ax.plot(grouped.index, grouped['mean'], \n",
    "                           label=strategy, marker='o', color=colors[i], linewidth=2)\n",
    "                    ax.fill_between(grouped.index, \n",
    "                                  grouped['mean'] - grouped['std'],\n",
    "                                  grouped['mean'] + grouped['std'],\n",
    "                                  alpha=0.3, color=colors[i])\n",
    "            \n",
    "            ax.set_xlabel('Anzahl Labels', fontsize=12)\n",
    "            ax.set_ylabel('Accuracy', fontsize=12)\n",
    "            ax.set_title(f'{clf}', fontsize=14, fontweight='bold')\n",
    "            ax.legend(loc='best', fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for idx in range(len(df['klassifizierer'].unique()), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "        \n",
    "        plt.suptitle(f'Learning Curves - Budget {budget:.1%}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/learning_curves_budget_{int(budget*100)}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Performance Heatmap\n",
    "    final_results = df.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget', 'run_id']).last().reset_index()\n",
    "    active_results = final_results[final_results['lernmodus'] == 'aktiv']\n",
    "    \n",
    "    # Erstelle Pivot-Tabelle für Heatmap\n",
    "    pivot_data = active_results.groupby(['klassifizierer', 'strategie', 'budget'])['accuracy'].mean().unstack(level=[1, 2])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Accuracy'}, ax=ax)\n",
    "    ax.set_title('Performance Heatmap: Klassifizierer × Strategie × Budget', fontsize=16)\n",
    "    ax.set_xlabel('Strategie × Budget', fontsize=12)\n",
    "    ax.set_ylabel('Klassifizierer', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Confusion Matrix für besten Klassifikator\n",
    "    best_config = active_results.groupby(['klassifizierer', 'strategie', 'budget'])['accuracy'].mean().idxmax()\n",
    "    logger.info(f\"Beste Konfiguration: {best_config}\")\n",
    "    \n",
    "    logger.info(f\"Visualisierungen gespeichert in '{output_dir}/'\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Hauptprogramm\n",
    "# -------------------------------------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Haupt-Einstiegspunkt:\n",
    "    1) Dachmaterial-Daten laden\n",
    "    2) Budget- und Strategie-Settings definieren\n",
    "    3) Experiment-Schleife\n",
    "    4) Ergebnisse speichern und zusammenfassen\n",
    "    \"\"\"\n",
    "    # Konfiguration\n",
    "    data_file = \"umrisse_with_all_data_and_shape_and_patch_and_normal.csv\"\n",
    "    target_column = \"mat_qgis\"\n",
    "    \n",
    "    # 1) Daten laden\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"ACTIVE LEARNING FÜR DACHMATERIAL-KLASSIFIKATION\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "         preprocessor, label_encoder, feature_names, n_classes) = load_dachmaterial_data(\n",
    "            filepath=data_file,\n",
    "            target_col=target_column,\n",
    "            test_size=0.2,\n",
    "            val_size=0.2,\n",
    "            random_state=SEED,\n",
    "            max_train=10000  # Kann erhöht werden für umfangreichere Experimente\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Datei '{data_file}' nicht gefunden!\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Laden der Daten: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) Experiment-Konfiguration\n",
    "    budgets = [0.3, 0.6, 0.9]  # 30%, 60%, 90% des Trainingssets\n",
    "    strategies = [\"least_confidence\", \"margin\", \"entropy\", \"information_density\"]\n",
    "    clf_keys = [\"TabularNN\", \"SVM\", \"RandomForest\", \"LogisticRegression\", \"NaiveBayes\"]\n",
    "    runs = 5  # Anzahl Wiederholungen pro Setting\n",
    "\n",
    "    logger.info(f\"\\nExperiment-Konfiguration:\")\n",
    "    logger.info(f\"Budgets: {budgets}\")\n",
    "    logger.info(f\"Strategien: {strategies}\")\n",
    "    logger.info(f\"Klassifikatoren: {clf_keys}\")\n",
    "    logger.info(f\"Runs pro Konfiguration: {runs}\")\n",
    "\n",
    "    # 3) Experiment-Schleife\n",
    "    all_results = []\n",
    "    total_experiments = len(strategies) * len(clf_keys) * len(budgets)\n",
    "    exp_count = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for budget in budgets:\n",
    "        for clf_key in clf_keys:\n",
    "            for strategy in strategies:\n",
    "                exp_count += 1\n",
    "                print(f\"\\n[{exp_count}/{total_experiments}] \"\n",
    "                      f\"Budget={budget:.1%}, Classifier={clf_key}, Strategy={strategy}\")\n",
    "\n",
    "                res = active_learning(\n",
    "                    X_train, y_train,\n",
    "                    X_val,   y_val,\n",
    "                    X_test,  y_test,\n",
    "                    strategy=strategy,\n",
    "                    clf_key=clf_key,\n",
    "                    budget=budget,\n",
    "                    runs=runs,\n",
    "                    n_classes=n_classes,\n",
    "                    batch_size=50\n",
    "                )\n",
    "                all_results.extend(res)\n",
    "\n",
    "    # 4) DataFrame bauen und als CSV ablegen\n",
    "    df = pd.DataFrame(\n",
    "        all_results,\n",
    "        columns=[\n",
    "            \"lernmodus\",      # \"passiv\" oder \"aktiv\"\n",
    "            \"klassifizierer\", # \"TabularNN\", \"SVM\", ...\n",
    "            \"strategie\",      # \"least_confidence\", ...\n",
    "            \"budget\",         # 0.3, 0.6, 0.9\n",
    "            \"run_id\",\n",
    "            \"zyklus\",\n",
    "            \"anz_label\",\n",
    "            \"accuracy\",\n",
    "            \"f1_macro\",\n",
    "            \"train_time\",\n",
    "            \"diversity\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    output_file = \"ergebnisse_dachmaterial_classification.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"\\nErgebnisse in '{output_file}' gespeichert.\")\n",
    "\n",
    "    # Zusammenfassung ausgeben\n",
    "    print_summary_statistics(df, label_encoder)\n",
    "    \n",
    "    # Visualisierungen erstellen\n",
    "    create_visualizations(df, label_encoder)\n",
    "    \n",
    "    # Gesamtzeit\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"\\nGesamte Laufzeit: {total_time/60:.1f} Minuten\")\n",
    "    \n",
    "    # Top-Performer identifizieren\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 5 KONFIGURATIONEN (nach durchschnittlicher Accuracy)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_results = df.groupby(['lernmodus', 'klassifizierer', 'strategie', 'budget', 'run_id']).last().reset_index()\n",
    "    active_results = final_results[final_results['lernmodus'] == 'aktiv']\n",
    "    \n",
    "    top_configs = active_results.groupby(['klassifizierer', 'strategie', 'budget']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'f1_macro': 'mean',\n",
    "        'train_time': 'mean'\n",
    "    }).round(4).sort_values('accuracy', ascending=False).head(5)\n",
    "    \n",
    "    print(top_configs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
